---
title: "06. Distributional semantics. Excercises"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

6.0 Используя поисковый интерфейс ДетКорпуса, соберите данные по
сочетаемости прилагательных с 5—10 любыми существительными по Вашему
выбору. Выгрузите статистику по сочетаемости прилагательных в формате
csv, исправьте заголовок файла и загрузите его на сервер.

6.1 По составленной вами статистике сочетаемости прилагательных с
существительными постройте график (biplot) проекции существительных и
25 самых частотных прилагательных на два главных компонента,
выделенных с помощью анализа соответствий (correspondense analysis). 
Hint: это повторение анализа в скрипте 06distributional-ssemantcs.Rmd
на Ваших данных. 

6.2 Используя любой корпус текстов (например, тот, с которым вы
работали в лабораторной работе, или любой из тех, что использовался в
примерах на практических занятиях) постройте матрицу термов-документов
и преобразуйте ее с помощью LSA в матрицу более низкой размерности
(например, 100). Не забудьте об удалении слишком редких слов,
пунктуации, возможно, стоп-слов. 

6.3 Выберите несколько релевантных для корпуса ключевых слов и
выведите списки ближайших соседей этих слов по LSA-трансформированной
матрице, используя косинусное расстояние (функция lsa::associate).

6.4 Примените к матрице термов-документов взвешивание частотностей по
схеме TF-IDF (quanteda::weight_tfidf) и преобразуйте взвешенную таким
образом матрицу с помощью LSA в матрицу более низкой размерности
(100). 

6.5 Выведите списки ближайших соседей тех же ключевых слов по этой
взвешенной матрице. Изменились ли они? Если да, то какой из вариантов
лучше отражает «семантическую близость»? В каком отношении?

6.6 Поэкспериментируйте с разными значениями размерности при
LSA-трансформации. Какие значения представляются Вам оптимальными с
точки зрения баланса между компактностью данных и сохранением
информации о семантических связях? Аргументируйте свой ответ,
приведите примеры.
