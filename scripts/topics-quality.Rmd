---
title: "Evaluating quality of LDA topics"
author: "Dari Batozhargalova"
date: "21 11 2021"
output: html_document
---

# Подготовка данных

```{r}
library(tidyverse)
library(ggplot2)
students = read.csv("topics-data.csv", encoding = "UTF-8")
students$topic_id = str_replace(students$topic_id, "з", "30")
students$topic_id = as.numeric(students$topic_id)
students$quality = str_to_lower(students$quality)
students$type = str_to_lower(students$type)
```

# Оценки

```{r}
# убираем знаки вопроса в разметке тем (если вопросом было размечено название темы полностью, то расцениваем как NA)
students$title = str_remove_all(students$title, "[?]")
students$title = ifelse(str_length(students$title) == 0, NA, students$title)

# считаем оценки по полноте разметки (итоговая оценка в колонке rounded)
grades = students %>% 
  group_by(user) %>% 
  summarise(NA_title = sum(is.na(title)), NA_quality = sum(is.na(quality)), NA_type = sum(is.na(type[quality != "good"])), NA_share = (NA_quality + NA_title + NA_type)/900, completeness = 1 - NA_share, grade = completeness*10, rounded = round(grade, 0))

grades

ggplot(grades) +
  geom_histogram(aes(grade)) +
  scale_y_continuous(breaks = 1:23) +
  theme_bw()
```

# Общая критичность разметчика

```{r}
# Посчитаем для каждого количество тем, качество которых студент_ка разметил_а как "bad" или "medium"
students %>% 
  filter(quality %in% c("bad", "medium")) %>%
  group_by(user) %>% 
  summarise(n = n(), criticism = n/300) %>% 
  arrange(desc(criticism))
```

# Inter-rater agreement (reliability) - согласованность оценок качества тем

Cohen's Kappa - статистика, которая используется для измерения межэкспертной надежности качественных элементов. Обычно считается, что это более надежная мера, чем простой расчет процента согласия, поскольку учитывает возможность совпадения случайно

```{r}
library(irr)

topic_quality = students %>% 
  select(1, 2, 6) %>% 
  ungroup() %>% 
      mutate(., quality = with(., case_when(
        quality == "bad" ~ "0",
        quality == "medium" ~ "0.5",
        quality == "good" ~ "1"
)))

topic_quality$quality = as.numeric(topic_quality$quality)

topic_quality = topic_quality %>% 
  pivot_wider(names_from = user, values_from = quality)
```

Посчитаем ICC (Inter-Class Correlation Coefficient) и Каппу - метрики для оценки IRR. Различаются типом данных, по которым их можно считать.

```{r}
icc(topic_quality[, c(2:24)], model="twoway", type="agreement", unit="average") # для непрерывных переменных с несколькими (больше 2) разметчиками

kappam.fleiss(topic_quality[, c(2:24)]) # для категориальных и ординальных переменных с несколькими (больше 2) разметчиками
```

Формулы игнорируют пропущенные значения (NA), но исключение/добавление разметчиков может влияет на уровень согласованности

```{r}
# сколько NA в оценках качества тем?
sum(is.na(students$quality))

# посмотрим, у кого стоит больше всего NA
students %>% 
  filter(is.na(quality) == TRUE) %>% 
  select(user) %>% 
  table() %>% 
  as.data.frame() %>% 
  arrange(desc(Freq))

# посмотрим, улучшит ли показатель согласованности исключение разметчиков, у которых было много NA
topic_quality = students %>% 
  filter(!(user %in% c("dnrepin", "ispetrov"))) %>% 
  select(1, 2, 6) %>% 
  ungroup() %>% 
      mutate(., quality = with(., case_when(
        quality == "bad" ~ "0",
        quality == "medium" ~ "0.5",
        quality == "good" ~ "1"
)))

topic_quality$quality = as.numeric(topic_quality$quality)

topic_quality = topic_quality %>% 
  pivot_wider(names_from = user, values_from = quality)

icc(topic_quality[, c(2:22)], model="twoway", type="agreement", unit="average")

kappam.fleiss(topic_quality[, c(2:22)])
```

# Inter-rater agreement (reliability) - согласованность разметок типа ошибок

```{r}
# сколько NA в разметках типа ошибок?
sum(is.na(students$type[students$quality != "good"]))

# посмотрим, у кого стоит больше всего NA
students %>% 
  filter(quality != "good") %>% 
  filter(is.na(type) == TRUE) %>% 
  select(user) %>% 
  table() %>% 
  as.data.frame() %>% 
  arrange(desc(Freq))

topic_type = students %>%
  select(1,2,7) %>%
  pivot_wider(names_from = user, values_from = type)
```

Посчитаем Каппу (двумя способами) - метрики для оценки IRR

```{r}
kappam.light(topic_type[, c(2:24)]) # для категориальных переменных с несколькими (больше 2) разметчиками

kappam.fleiss(topic_type[, c(2:24)]) # для категориальных и ординальных переменных с несколькими (больше 2) разметчиками
```

# Корреляции 

Насколько средняя оценка качества тем коррелирует с формальными показателями?

```{r}
data = students %>% 
  select(topic_id, quality)
data = na.omit(data)
data$quality = ifelse(data$quality == "bad", 0,
                      ifelse(data$quality == "medium", 0.5, 1))
data = data %>% 
  group_by(topic_id) %>% 
  summarise(mean_quality = mean(quality))
```

```{r}
library(xml2)
diag = read_xml("diag300.xml")
links = xml_find_all(diag, "//topic")
topics = bind_rows(lapply(xml_attrs(links), function(x) data.frame(as.list(x), stringsAsFactors=FALSE)))
topics$id = rep(1:300)
topics[,2:13] = apply(topics[, 2:13], 2, as.numeric)
topics$mean_quality = data$mean_quality
```

```{r}
library(sjPlot)
tab_corr(topics)
```

# Регрессионная модель

Предсказываем среднюю оценку качества тем по формальным показателям

```{r}
model = lm(mean_quality ~ .-id, data = topics)
summary(model)
```
